{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Seminar 3: Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Vadim Kokhtev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this seminar, we will discuss gradient descent, its modifications, and the selection of parameters in them.\n",
    "\n",
    "The notebook uses the `plotly` library, which allows you to create interactive charts. These charts are not displayed in `nbviewer` or on github, so for easy viewing, it is better to download the notebook, open it locally, and restart it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Recall that in gradient descent, the values of the parameters at the next step are obtained from the values of the parameters at the previous step by shifting towards the antigradient of the functional:\n",
    "\n",
    "$$w^{(t)} = w^{(t-1)} - \\eta_t \\nabla Q(w^{(t-1)}),$$\n",
    "$\\eta_t$ — learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic computational complexity\n",
    "\n",
    "The optimal set of weights for linear regression from the MSE point of view looks like $w = (X^TX)^{-1}X^Ty$. In this formula, there is an inversion of the matrix $X^TX$ - a very time-consuming operation with a large number of features. It is not difficult to calculate that the computational complexity is $O (d^3 + d^2 \\ell)$. In real tasks this complexity is often unacceptable, so the parameters are searched for by iterative methods, the cost of which is less. One of them is gradient descent.\n",
    "\n",
    "The MSE gradient is written as:\n",
    "\n",
    "$$\\nabla Q(w) = 2X^T(Xw - y).$$\n",
    "\n",
    "The complexity of the calculations in this case is $O (d \\ell)$. Stochastic gradient descent differs from simple gradient descent by replacing the gradient with an unbiased estimate for one or more objects. In this case, the complexity becomes $O(kd)$, where $k$ is the number of objects for which the gradient is evaluated, $k \\ll \\ell$. This partly explains the popularity of stochastic optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of GD and SGD trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a objects-feature matrix $X$ and a vector of weights $w_{true}$, calculate the vector of target variables $y$ as $Xw_{true}$ and add normal noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.422329Z",
     "start_time": "2020-09-23T08:57:23.280579Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.436737Z",
     "start_time": "2020-09-23T08:57:23.424779Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "np.random.seed(100)\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]  # for different scales\n",
    "\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "\n",
    "w_0 = np.random.uniform(-1, 1, (n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters of the gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.442606Z",
     "start_time": "2020-09-23T08:57:23.439373Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a linear regression for MSE using the obtained data using full gradient descent — thus we will get a vector of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.452907Z",
     "start_time": "2020-09-23T08:57:23.445041Z"
    }
   },
   "outputs": [],
   "source": [
    "step_size = 1e-2\n",
    "\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= 2 * step_size * np.dot(X.T, np.dot(X, w) - Y) / Y.shape[0]\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.469366Z",
     "start_time": "2020-09-23T08:57:23.454538Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def compute_limits(w_list):\n",
    "    dx = np.max(np.abs(w_list[:, 0] - w_true[0])) * 1.1\n",
    "    dy = np.max(np.abs(w_list[:, 1] - w_true[1])) * 1.1\n",
    "    \n",
    "    return (w_true[0] - dx, w_true[0] + dx), (w_true[1] - dy, w_true[1] + dy)\n",
    "\n",
    "\n",
    "def compute_levels(w_list, x_range, y_range, num: int = 100):\n",
    "    x, y = np.linspace(x_range[0], x_range[1], num), np.linspace(y_range[0], y_range[1], num)\n",
    "    A, B = np.meshgrid(x, y)\n",
    "\n",
    "    levels = np.empty_like(A)\n",
    "\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "            \n",
    "    return x, y, levels\n",
    "\n",
    "\n",
    "def make_contour(x, y, levels, name: str=None):\n",
    "    return go.Contour(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=levels,\n",
    "        contours_coloring='lines',\n",
    "        line_smoothing=1,\n",
    "        line_width=2,\n",
    "        ncontours=100,\n",
    "        opacity=0.5,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "def make_arrow(figure, x, y):\n",
    "    x, dx = x\n",
    "    y, dy = y\n",
    "    \n",
    "    figure.add_annotation(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        ax=x + dx,\n",
    "        ay=y + dy,\n",
    "        xref='x',\n",
    "        yref='y',\n",
    "        text='',\n",
    "        showarrow=True,\n",
    "        axref = 'x',\n",
    "        ayref='y',\n",
    "        arrowhead=2,\n",
    "        arrowsize=1,\n",
    "        arrowwidth=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_trajectory(w_list, name):\n",
    "    # compute limits\n",
    "    x_range, y_range = compute_limits(w_list)\n",
    "    \n",
    "    # compute level set\n",
    "    x, y, levels = compute_levels(w_list, x_range, y_range)\n",
    "    \n",
    "    # plot levels\n",
    "    contour = make_contour(x, y, levels, 'Loss function levels')\n",
    "\n",
    "    # plot weights\n",
    "    w_path = go.Scatter(\n",
    "        x=w_list[:, 0][:-1],\n",
    "        y=w_list[:, 1][:-1],\n",
    "        mode='lines+markers',\n",
    "        name='W',\n",
    "        marker=dict(size=7, color='red')\n",
    "    )\n",
    "\n",
    "    # plot final weight\n",
    "    w_final = go.Scatter(\n",
    "        x=[w_list[:, 0][-1]],\n",
    "        y=[w_list[:, 1][-1]],\n",
    "        mode='markers',\n",
    "        name='W_final',\n",
    "        marker=dict(size=10, color='black'),\n",
    "    )\n",
    "    \n",
    "    # plot true optimum    \n",
    "    w_true_point = go.Scatter(\n",
    "        x=[w_true[0]],\n",
    "        y=[w_true[1]],\n",
    "        mode='markers',\n",
    "        name='W_true',\n",
    "        marker=dict(size=10, color='black'),\n",
    "        marker_symbol='star'\n",
    "    )\n",
    "    \n",
    "    # make the figure\n",
    "    fig = go.Figure(data=[contour, w_path, w_final, w_true_point])\n",
    "\n",
    "    fig.update_xaxes(type='linear', range=x_range)\n",
    "    fig.update_yaxes(type='linear', range=y_range)\n",
    "\n",
    "    fig.update_layout(title=name)\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=True,\n",
    "        width=700,\n",
    "        margin=dict(\n",
    "            l=50,\n",
    "            r=50,\n",
    "            b=50,\n",
    "            t=100,\n",
    "            pad=4\n",
    "        ),\n",
    "        paper_bgcolor='LightSteelBlue',\n",
    "    )\n",
    "\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ))\n",
    "\n",
    "    fig.update_traces(showlegend=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:23.942134Z",
     "start_time": "2020-09-23T08:57:23.471316Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_trajectory(w_list, 'Gradient descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the course of calculus, it is known that the gradient is perpendicular to the level lines. This explains chaotic trajectories of gradient descent. For greater clarity, at each point in the space, we calculate the gradient of the functional and show its direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:24.971170Z",
     "start_time": "2020-09-23T08:57:23.943882Z"
    }
   },
   "outputs": [],
   "source": [
    "# make new figure with contour lines\n",
    "x_range, y_range = compute_limits(w_list)\n",
    "x, y, levels = compute_levels(w_list, x_range, y_range)\n",
    "contour = make_contour(x, y, levels, 'Loss function levels')\n",
    "fig = go.Figure(data=[contour])\n",
    "\n",
    "# visualize the gradients\n",
    "\n",
    "x_smol, y_smol, _ = compute_levels(w_list, x_range, y_range, num=10)\n",
    "x_smol, y_smol = x_smol[1:-1], y_smol[1:-1]\n",
    "A_smol, B_smol = np.meshgrid(x_smol, y_smol)\n",
    "\n",
    "for i in range(A_smol.shape[0]):\n",
    "    for j in range(A_smol.shape[1]):\n",
    "        w_tmp = np.array([A_smol[i, j], B_smol[i, j]])\n",
    "        \n",
    "        antigrad = 0.003 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n",
    "        \n",
    "        make_arrow(fig, (A_smol[i, j], antigrad[0]), (B_smol[i, j], antigrad[1]))\n",
    "        \n",
    "\n",
    "fig.update_xaxes(type='linear', range=x_range)\n",
    "fig.update_yaxes(type='linear', range=y_range)\n",
    "\n",
    "fig.update_layout(title = 'Antigradient')\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    margin=dict(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "    paper_bgcolor='LightSteelBlue',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the trajectories of stochastic gradient descent, repeating the same steps, while evaluating the gradient from the subsample of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:24.977605Z",
     "start_time": "2020-09-23T08:57:24.974273Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_grad_on_batch(X, Y, w, batch_size):\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    return 2 * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:24.989113Z",
     "start_time": "2020-09-23T08:57:24.980201Z"
    }
   },
   "outputs": [],
   "source": [
    "step_size = 1e-2\n",
    "\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= step_size * calc_grad_on_batch(X, Y, w, batch_size)\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:25.336711Z",
     "start_time": "2020-09-23T08:57:24.990946Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_trajectory(w_list, 'Stochastic gradient descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stochastic gradient method \"wanders\" around the optimum. This is explained by the selection of the gradient descent step $ \\ eta_k$. The point is that for the convergence of stochastic gradient descent for the sequence of steps $ \\eta_k$ , the [Robbins-Monroe conditions](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586) must be satisfied:\n",
    "$$\n",
    "\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n",
    "$$\n",
    "Intuitively, this means the following: (1) the sequence must diverge so that the optimization method can reach any point in space, (2) but not diverge too quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to look at the SGD trajectories, the sequence of steps of which satisfies the Robbins-Monroe conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:25.345454Z",
     "start_time": "2020-09-23T08:57:25.340581Z"
    }
   },
   "outputs": [],
   "source": [
    "step_size_0 = 0.01\n",
    "\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_0 / (i+1)\n",
    "    w -= step_size * calc_grad_on_batch(X, Y, w, batch_size)\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:25.681557Z",
     "start_time": "2020-09-23T08:57:25.347110Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_trajectory(w_list, 'Stochastic gradient descent with dynamic learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the gradient descent moves more directionally, but does not reach the optimum. Let's try a more complex scheme for changing the step length:\n",
    "$$\n",
    "    \\eta_t\n",
    "    =\n",
    "    \\lambda\n",
    "    \\left(\n",
    "        \\frac{s_0}{s_0 + t}\n",
    "    \\right)^p.\n",
    "$$\n",
    "Let's use $s_0 = 1$ and experiment with different $\\lambda$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:25.686426Z",
     "start_time": "2020-09-23T08:57:25.683001Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd_with_lr_schedule(lambda_param, p=0.5, s_init=1.0, batch_size=10):\n",
    "    w = w_0.copy()\n",
    "    w_list = [w.copy()]\n",
    "\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        step_size = lambda_param * np.power(s_init / (s_init + i), p)\n",
    "        w -= step_size * calc_grad_on_batch(X, Y, w, batch_size)\n",
    "        w_list.append(w.copy())\n",
    "\n",
    "    return np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:26.056892Z",
     "start_time": "2020-09-23T08:57:25.687887Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.8)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:26.374790Z",
     "start_time": "2020-09-23T08:57:26.058822Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.5)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:26.697104Z",
     "start_time": "2020-09-23T08:57:26.376649Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the coefficients in the formula for the step length are hyperparameters, and they need to be selected. It is advisable to use a validation sample for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the size of the sub-sample used to estimate the gradient affects the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:27.012243Z",
     "start_time": "2020-09-23T08:57:26.699287Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35, batch_size=1)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:27.341491Z",
     "start_time": "2020-09-23T08:57:27.013536Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35, batch_size=10)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:27.640742Z",
     "start_time": "2020-09-23T08:57:27.342796Z"
    }
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35, batch_size=100)\n",
    "plot_trajectory(w_list, 'SGD with learning rate schedule')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion, in general, is obvious: the larger the subsample size, the more stable the gradient descent path. It is more interesting to see how this affects the rate of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of convergence rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study how quickly the methods of full and stochastic gradient descent reach the optimum. Let's generate a sample and plot the dependence of the functional on the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:27.645177Z",
     "start_time": "2020-09-23T08:57:27.642795Z"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:27.667402Z",
     "start_time": "2020-09-23T08:57:27.647108Z"
    }
   },
   "outputs": [],
   "source": [
    "# data generation\n",
    "n_features = 50\n",
    "n_objects = 10000\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, n_features)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:28.230225Z",
     "start_time": "2020-09-23T08:57:27.669215Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import norm\n",
    "\n",
    "step_size_sgd = 1e-2\n",
    "step_size_gd = 1e-2\n",
    "\n",
    "w_sgd = np.random.uniform(-1, 1, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]\n",
    "\n",
    "norm_sgd = []\n",
    "norm_gd = []\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_sgd / ((i+1) ** 0.4)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    \n",
    "    w_sgd -= step_size * calc_grad_on_batch(X, Y, w_sgd, batch_size)\n",
    "    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2)))\n",
    "    norm_sgd.append(norm(np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample])))\n",
    "    \n",
    "    w_gd -= 2 * step_size_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n",
    "    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2)))\n",
    "    norm_gd.append(norm(np.dot(X.T, np.dot(X, w_gd) - Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:28.276911Z",
     "start_time": "2020-09-23T08:57:28.238273Z"
    }
   },
   "outputs": [],
   "source": [
    "full_gd = go.Scatter(x=np.arange(num_steps+1), y=residuals_gd, name='Full GD')\n",
    "sgd = go.Scatter(x=np.arange(num_steps+1), y=residuals_sgd, name='SGD')\n",
    "\n",
    "fig = go.Figure(data=[full_gd, sgd])\n",
    "\n",
    "fig.update_xaxes(type='linear', range=[-1, num_steps + 1])\n",
    "fig.update_yaxes(type='linear')\n",
    "\n",
    "fig.update_layout(title = 'Residuals comparison', xaxis=dict(title=\"Iteration\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    margin=dict(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "    paper_bgcolor='LightSteelBlue',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:28.317200Z",
     "start_time": "2020-09-23T08:57:28.280776Z"
    }
   },
   "outputs": [],
   "source": [
    "full_gd = go.Scatter(x=np.arange(num_steps+1), y=norm_gd, name='Full GD')\n",
    "sgd = go.Scatter(x=np.arange(num_steps+1), y=norm_sgd, name='SGD')\n",
    "\n",
    "fig = go.Figure(data=[full_gd, sgd])\n",
    "\n",
    "fig.update_xaxes(type='linear', range=[-1, num_steps + 1])\n",
    "fig.update_yaxes(type='linear')\n",
    "\n",
    "fig.update_layout(title = 'Gradient norm comparison', xaxis=dict(title=\"Iteration\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    margin=dict(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "    paper_bgcolor='LightSteelBlue',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, GD is close to the optimum in just a few iterations, while the behavior of SGD can be very unstable. As a rule, for more complex models, even greater fluctuations are observed depending on the quality of the functional on the iteration when using stochastic gradient methods. By selecting the step size, you can achieve a better convergence rate, and there are methods that adaptively select the step size (AdaGrad, Adam, RMSProp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to see how much the use of mini-batch GD accelerates convergence. Let's calculate in how many steps the stochastic gradient descent approaches the true solution quite strongly, depending on the size of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:31.162392Z",
     "start_time": "2020-09-23T08:57:28.318622Z"
    }
   },
   "outputs": [],
   "source": [
    "step_size_sgd = 1e-2\n",
    "step_size_gd = 1e-2\n",
    "num_steps = 500\n",
    "\n",
    "w_init = np.random.uniform(-1, 1, n_features)\n",
    "w_gd = w_init.copy()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w_gd -= 2 * step_size_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n",
    "    \n",
    "best_error = np.mean(np.power(np.dot(X, w_gd) - Y, 2))\n",
    "steps_before_conv = []\n",
    "batch_sizes = np.arange(0, 500, 10)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    w_sgd = w_init.copy()\n",
    "    for i in range(num_steps):\n",
    "        step_size = step_size_sgd / ((i+1) ** 0.4)\n",
    "        sample = np.random.randint(n_objects, size=batch_size)\n",
    "\n",
    "        w_sgd -= step_size * calc_grad_on_batch(X, Y, w_sgd, batch_size)\n",
    "        err = np.mean(np.power(np.dot(X, w_sgd) - Y, 2))\n",
    "        if np.abs(err - best_error) < 1:\n",
    "            break\n",
    "        \n",
    "    steps_before_conv.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T08:57:31.210105Z",
     "start_time": "2020-09-23T08:57:31.179977Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_speed = go.Scatter(x=batch_sizes, y=steps_before_conv, name='Number of steps to convergence')\n",
    "\n",
    "fig = go.Figure(data=conv_speed)\n",
    "\n",
    "fig.update_layout(title='Convergence speed',\n",
    "                 xaxis=dict(title=\"batch size\"),\n",
    "                yaxis=dict(title=\"steps before convergence\")\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=700,\n",
    "    margin=dict(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=100,\n",
    "        pad=4\n",
    "    ),\n",
    "    paper_bgcolor='LightSteelBlue',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that specifically on this data set, increasing the size of the batch to about 100 allows for a significant acceleration of convergence. At the same time, increasing the batch by a factor of 100 also leads to a proportional deceleration of each step of the gradient descent. Therefore, it usually makes sense to estimate the gradient from a small subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "\n",
    "def calc_grad(x, y):\n",
    "\n",
    "    z = lambda xy: np.sin(0.5*xy[0]**2-0.25*xy[1]**2-1)*np.cos(2*xy[0] + 1 - np.exp(xy[1]))*xy[0]\n",
    "    dz = nd.Gradient(z)\n",
    "    dz_dx, dz_dy = dz([x, y])\n",
    "    return [dz_dx, dz_dy]\n",
    "\n",
    "\n",
    "def f(x,y):\n",
    "    return np.sin(0.5*x**2-0.25*y**2-1)*np.cos(2*x + 1 - np.exp(y))*x\n",
    "\n",
    "\n",
    "def calc_gd_trajectory(x_init = 1.17, y_init = -1.73, alpha = 0.1, n_steps = 100):\n",
    "    \n",
    "    gd_trajectory_x = [x_init]\n",
    "    gd_trajectory_y = [y_init]\n",
    "    gd_trajectory_z = [f(x_init, y_init)+0.1]\n",
    "    gd_steps =['Initial point']\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        grad_val = calc_grad(gd_trajectory_x[-1], gd_trajectory_y[-1])\n",
    "        gd_trajectory_x.append(gd_trajectory_x[-1] - alpha*grad_val[0])\n",
    "        gd_trajectory_y.append(gd_trajectory_y[-1] - alpha*grad_val[1])\n",
    "        gd_trajectory_z.append(f(gd_trajectory_x[-1]\n",
    "                                               , gd_trajectory_y[-1])+0.1)\n",
    "\n",
    "        gd_steps.append(str(i) + ' step')\n",
    "        \n",
    "    return gd_trajectory_x, gd_trajectory_y, gd_trajectory_z, gd_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha should not be greater than 0.5\n",
    "\n",
    "gd_trajectory_x, gd_trajectory_y, gd_trajectory_z, gd_steps = calc_gd_trajectory(x_init=2.54\n",
    "                                                                                 , y_init=-1.25 \n",
    "                                                                                 ,alpha=0.2\n",
    "                                                                                 ,n_steps=100)\n",
    "x_grid = np.linspace(-4, 4, 100)\n",
    "y_grid = np.linspace(-4, 4, 100)\n",
    "\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "Z = (np.sin(0.5*X**2 - 0.25*Y**2-1)*np.cos(2*X + 1 - np.exp(Y)))*X\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(x=X, y=Y, z=Z, colorscale='gnbu')], )\n",
    "fig.data[-1].name = 'Surface'\n",
    "fig.add_scatter3d(x=gd_trajectory_x, y=gd_trajectory_y\n",
    "                  , z=gd_trajectory_z, marker_size = 3, hovertext=gd_steps)\n",
    "\n",
    "fig.data[-1].name = 'Descent trajectory'\n",
    "fig.update_layout(\n",
    "    title='Loss function surface plot',\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    width=1200, height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture above you can see the graph of the function \n",
    "\n",
    "\n",
    "$$F(x,y) = sin(\\frac{1}{2}x^2-\\frac{1}{4}y^2-1)\\cdot cos(2x+1-e^y)\\cdot x$$\n",
    "\n",
    "\n",
    "It can be seen that this surface has a complex topology with a large number of local minima.\n",
    "For example loss functions of some neural networks may have such a landscape\n",
    "\n",
    "You can zoom in and out, rotate and enjoy!\n",
    "\n",
    "If you want to open it in a new window, the following line will help you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"surface.html\", auto_open=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
